3 FSST+ Design and Methodology
3.1 Methodology Overview
The development of FSST+ will proceed in clearly defined phases—starting with a standalone proto-
type and eventually integrating the solution into DuckDB (a stretch goal). The key components of the
methodology are as follows.
• Batching and Truncation:
– Divide target columns into runs of 128 strings.
– Truncate each string to 120 bytes, ensuring that even if escape sequences double the size (up
to 240 bytes), the length information remains storable in a single uint8 . In the worst-case
scenario (128 strings each truncated to 120 bytes), the total size is approximately 15.4 KB,
which fits within typical L1 cache limits (16–128 KB per core).
• Sorting and Grouping:
– Sort the 128 now truncated strings, so that similar strings appear consecutively.
– Segment the sorted data into similarity chunks and identify an optimal split point for each
chunk—separating a common prefix from the variable suffixes.
– Note: A heuristic or simple cost model (possibly using metadata captured during the
sorting process) will be investigated to determine reliable boundaries for similarity chunks
and the corresponding split points.
• Dual Symbol Tables and Jumpback Mechanism:
– Construct two symbol tables using FSST: one for the common prefixes and one for the
suffixes.
– Employ a jumpback mechanism where each compressed string stores a single, typically
two-byte, pointer referencing its prefix, thereby reducing pointer overhead.
• Experimental Designs:
– Assess whether capturing redundancy not only at the start of strings, but also at the end
of strings (by processing their reversed versions and using an is reversed flag) further
improves compression.
– Benchmark an alternative design, where the prefix length is stored at the beginning of the
prefix data area instead of at the start of each suffix. In this case a zero jumpback offset
would indicate the absence of a prefix.
3.2 Data Structure Design
The efficiency of random access and pointer overhead minimization will be ensured by the following
data layout:
• Block Header and Run Offsets: Data is segmented into blocks subdivided into runs (each
containing 128 strings). An array (e.g., run start offsets[] ) records the starting offset for
each run.
• Run Header: Within each run, we have a Run Header. This is because we need to know the
delimiters of each compressed string within the run.
Therefore, Run Header starts with base offset which points to the start of the compressed
strings, followed by num strings indicating how many compressed string elements there are,
and an array (e.g., string offsets[] ) that maps each string in the run to its compressed
location.
2
Figure 1: Data Structure Overview
• Prefix Data Area: Contains encoded prefixes[] , a contiguous region storing all compressed
prefixes.
• Compressed Strings: Each string starts starts with 8 bytes prefix length . In the case
prefix length is not zero, it will be followed by 16 bytes jumpback offset , along with
3
encoded suffix . When prefix length is zero, it’s directly followed by encoded suffix
with no jumpback offset in between. That saves precious compression space.
A uint16 jumpback offset allows us to save values from 0 to 65,536. What this means is the
maximum range we can jump back is 65,536 bytes to find our prefix, so the number bytes stored
from the Prefix Data Area onwards, up to the last jump-back offset shouldn’t exceed 65,536.
3.3 Evaluation
The standalone prototype will be evaluated using the following metrics:
• Compression Ratio: FSST+ will be compared to FSST, LZ4, zstd, and dictionary encod-
ing on large datasets with string data. These include NextiaJD; RealNest; Redset; Public BI
Benchmark; and ClickBench.
• Compression/Decompression Speed: The evaluation will ensure that the improved com-
pression does not compromise the system’s fast, random-access capabilities, and mantains an
acceptable compression / decompression speed.